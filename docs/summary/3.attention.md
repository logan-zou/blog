# 注意力机制

## 一、注意力层

### 1. 基础理论

1. 我们可以把简单的自注意力层看作一个黑盒，n 个元素的序列输出自注意力层，则得到 n 个元素的输出序列，通过自注意力层可以考虑到整个序列的内容。

2. 自注意力的计算：
    
    假设输入为 a = (a1, a2, a3, a4)，输出应为 b = (b1, b2, b3, b4)，以 a1 到 b1 的计算过程为例：

    ① 首先计算 a1 与其他每一个元素（例如a2）的注意力系数，即 $q_1 = W_q * a_1$， $k_2 = W_k * a_2$，$\alpha_{12} = sum(q_1 * k_2)$。

    ② 对所有注意力系数做 Softmax，即 $\alpha_{12}^{'} = exp(\alpha_{12}) / \sum_{j}exp(\alpha_{1j})$

    ③ 基于源输入计算 value，即 $v_1 = W_v * a_1$

    ④ 值与注意力系数进行加权求和，得到 $b_1 = \sum_{i}\alpha_{1i}^{'}v_{i}$

3. 从矩阵的角度：

    输入矩阵 $I = (a_1, a_2, a_3...a_n) \in R^{i\times j}$

    参数矩阵 $W_q, W_k, W_v \in R^{j \times k}$
    
    查询值矩阵 $Q = I * W_q \in R^{i \times k}$，键值矩阵 $K = I * W_k$，值矩阵 $V = I * W_v$

    接着计算注意力系数 $A = QK^T \in R^{i \times i}$，$A^{'} = Softmax(A)$。

    最后计算输出矩阵 $O = A^{'}*V = (b_1, b_2...b_n) \in R^{i\times k}$。

### 2. 手写 Attention

1. attention 计算：

```python
def attention(query, key, value, dropout=None):
    # attention 的手写计算
    d_k = query.size(-1) # 获取键向量的维度，键向量的维度和值向量的维度相同
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
    # 计算Q与K的内积并除以根号dk
    # transpose——相当于转置
    p_attn = scores.softmax(dim=-1)
    if dropout is not None:
        p_attn = dropout(p_attn)
        # 采样
    return torch.matmul(p_attn, value), p_attn
    # 根据计算结果对value进行加权求和
```

2. 实现一个注意力层：

```python
import torch.nn as nn
class Attention_layer(nn.Module):
    # 注意力层
    def __init__(self, d_k : int, d_input : int):
        self.d_k = d_k
        self.d_input = d_input
        # 三个参数矩阵其实可以看做三个线性层，最后输出结果再经过一个全连接
        self.linears = [nn.Linear(d_input, d_k) for i in range(4)]

    def forward(self, query, key, value):
        # 前向计算
        # 此处考虑 batch_size
        # 那么输入维度应该是 batch_size * length * d_input
        batch_size = query.size(0)
        # 计算三个输入
        # 单头注意力应该不需要做变形，线性层输出 batch_size * length * d_k
        query, key, value = [linear(x) for linear, x in zip(self.linears, (query, key, value))]
        # 进行注意力运算
        x, self.attn = attention(query, key, value)
        # 注意力输出 batch_size * length * d_k
        # 经过全连接后输出
        del query, key, value
        # x = x.transpose(1,2)
        # print(x.size())
        return self.linears[-1](x)
```

## 二、多头注意力层

### 1. 基础理论

多头注意力机制其实就是在注意力机制的基础上增加参数量，即 n 个头则有 n 个 $W_q$、$W_k$、$W_v$，然后每个头对应的参数自己计算，最后拼接在一起变形后经过全连接网络输出即可。

### 2. 手写多头注意力层

注：Attention 计算可以沿用上文，我们直接基于 Attention 机制搭建多头注意力层即可（多头注意力使用了很多矩阵技巧，背后逻辑相对复杂）：

```python
class Multi_head_attention_layer(nn.Module):

    def __init__(self, n_head : int, d_k : int, d_input : int):
        super(Multi_head_attention_layer, self).__init__()
        # 输出维度应该要能够被头数整除，否则拼接不出来
        assert d_k % n_head == 0
        self.n_head = n_head
        self.d_k = d_k
        self.d_input = d_input
        # 应该初始化3个线性层，每个线性层都是 n_head 个参数矩阵的拼接
        self.linears = [nn.Linear(d_input, d_k) for i in range(3)]
        # 最后一个全连接额外初始化
        self.fnn = nn.Linear(d_k, d_k)

    def forward(self, query, key, value):
        # 每个头处理的维度应该是 d_k // n_head
        d_k_head = self.d_k // self.n_head
        batch_size = query.size(0)
        # 输入经过线性层得到多个头q、k、v的拼接，线性层输入为 batch_size * length * d_input，输出为 batch_size * length * d_k
        # 由于要经过多头注意力处理，需要将输出变形为 batch_size * length * n_head * d_k_head
        # 由于接下来要进入注意力层，转置的是后面两个维度，所以要再变形一下，把 n_head 放前面去
        query, key, value = [linear(x).view(batch_size, -1, self.n_head, d_k_head).transpose(1,2) for linear, x in zip(self.linears, (query, key, value))]
        # 进入注意力层计算，注意力层输出为 batch_size * n_head * length * d_k_head
        x, self.atten = attention(query, key, value)
        # 需要将注意力层输出拼接成 batch_size * length * d_k 的形状，即回归之前的形状，逆操作一下
        # Pytorch 设置如果 transpose 后 view 会报错（因为view直接基于底层数组），需要调用 contiguous 来重新开辟一段内存
        x = x.transpose(1,2).contiguous().view(batch_size, -1, self.d_k)
        del query, key, value
        return self.fnn(x)
```

