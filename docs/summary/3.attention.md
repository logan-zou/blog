# 注意力机制

## 一、注意力层

### 1. 基础理论

1. 我们可以把简单的自注意力层看作一个黑盒，n 个元素的序列输出自注意力层，则得到 n 个元素的输出序列，通过自注意力层可以考虑到整个序列的内容。

2. 自注意力的计算：
    
    假设输入为 a = (a1, a2, a3, a4)，输出应为 b = (b1, b2, b3, b4)，以 a1 到 b1 的计算过程为例：

    ① 首先计算 a1 与其他每一个元素（例如a2）的注意力系数，即 $q_1 = W_q * a_1$， $k_2 = W_k * a_2$，$\alpha_{12} = sum(q_1 * k_2)$。

    ② 对所有注意力系数做 Softmax，即 $\alpha_{12}^{'} = exp(\alpha_{12}) / \sum_{j}exp(\alpha_{1j})$

    ③ 基于源输入计算 value，即 $v_1 = W_v * a_1$

    ④ 值与注意力系数进行加权求和，得到 $b_1 = \sum_{i}\alpha_{1i}^{'}v_{i}$

3. 从矩阵的角度：

    输入矩阵 $I = (a_1, a_2, a_3...a_n) \in R^{i\times j}$

    参数矩阵 $W_q, W_k, W_v \in R^{j \times k}$
    
    查询值矩阵 $Q = I * W_q \in R^{i \times k}$，键值矩阵 $K = I * W_k$，值矩阵 $V = I * W_v$

    接着计算注意力系数 $A = QK^T \in R^{i \times i}$，$A^{'} = Softmax(A)$。

    最后计算输出矩阵 $O = A^{'}*V = (b_1, b_2...b_n) \in R^{i\times k}$。

4. CNN 与 Attention 对比：CNN 是 Attention 的特例，如果感受野范围并非人为划定而是由模型学习得到，那就是 Attention。数据量少时，适合 CNN；数据量多时，适合 Attention。

5. RNN 与 Attention 对比：① 对于 RNN，如果最右边要考虑最左边的输入，它就必须把最左边的输入存在记忆里面，才能不“忘掉”，一路带到最右边，才能够在最后一个时间点被考虑；但 Attention 输出一个查询，输出一个键，只要它们匹配得起来，“天涯若比邻”。自注意力可以轻易地从整个序列上非常远的向量抽取信息。② RNN 必须按顺序串行计算，Attention 额可以很轻松地实现并行化。


### 2. 手写 Attention

1. attention 计算：

```python
def attention(query, key, value, dropout=None):
    # attention 的手写计算
    d_k = query.size(-1) # 获取键向量的维度，键向量的维度和值向量的维度相同
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
    # 计算Q与K的内积并除以根号dk
    # transpose——相当于转置
    p_attn = scores.softmax(dim=-1)
    if dropout is not None:
        p_attn = dropout(p_attn)
        # 采样
    return torch.matmul(p_attn, value), p_attn
    # 根据计算结果对value进行加权求和
```

2. 实现一个注意力层：

```python
import torch.nn as nn
class Attention_layer(nn.Module):
    # 注意力层
    def __init__(self, d_k : int, d_input : int):
        self.d_k = d_k
        self.d_input = d_input
        # 三个参数矩阵其实可以看做三个线性层，最后输出结果再经过一个全连接
        self.linears = [nn.Linear(d_input, d_k) for i in range(4)]

    def forward(self, query, key, value):
        # 前向计算
        # 此处考虑 batch_size
        # 那么输入维度应该是 batch_size * length * d_input
        batch_size = query.size(0)
        # 计算三个输入
        # 单头注意力应该不需要做变形，线性层输出 batch_size * length * d_k
        query, key, value = [linear(x) for linear, x in zip(self.linears, (query, key, value))]
        # 进行注意力运算
        x, self.attn = attention(query, key, value)
        # 注意力输出 batch_size * length * d_k
        # 经过全连接后输出
        del query, key, value
        # x = x.transpose(1,2)
        # print(x.size())
        return self.linears[-1](x)
```

## 二、多头注意力层

### 1. 基础理论

多头注意力机制其实就是在注意力机制的基础上增加参数量，即 n 个头则有 n 个 $W_q$、$W_k$、$W_v$，然后每个头对应的参数自己计算，最后拼接在一起变形后经过全连接网络输出即可。

### 2. 手写多头注意力层

注：Attention 计算可以沿用上文，我们直接基于 Attention 机制搭建多头注意力层即可（多头注意力使用了很多矩阵技巧，背后逻辑相对复杂）：

```python
class Multi_head_attention_layer(nn.Module):

    def __init__(self, n_head : int, d_k : int, d_input : int):
        super(Multi_head_attention_layer, self).__init__()
        # 输出维度应该要能够被头数整除，否则拼接不出来
        assert d_k % n_head == 0
        self.n_head = n_head
        self.d_k = d_k
        self.d_input = d_input
        # 应该初始化3个线性层，每个线性层都是 n_head 个参数矩阵的拼接
        self.linears = [nn.Linear(d_input, d_k) for i in range(3)]
        # 最后一个全连接额外初始化
        self.fnn = nn.Linear(d_k, d_k)

    def forward(self, query, key, value):
        # 每个头处理的维度应该是 d_k // n_head
        d_k_head = self.d_k // self.n_head
        batch_size = query.size(0)
        # 输入经过线性层得到多个头q、k、v的拼接，线性层输入为 batch_size * length * d_input，输出为 batch_size * length * d_k
        # 由于要经过多头注意力处理，需要将输出变形为 batch_size * length * n_head * d_k_head
        # 由于接下来要进入注意力层，转置的是后面两个维度，所以要再变形一下，把 n_head 放前面去
        query, key, value = [linear(x).view(batch_size, -1, self.n_head, d_k_head).transpose(1,2) for linear, x in zip(self.linears, (query, key, value))]
        # 进入注意力层计算，注意力层输出为 batch_size * n_head * length * d_k_head
        x, self.atten = attention(query, key, value)
        # 需要将注意力层输出拼接成 batch_size * length * d_k 的形状，即回归之前的形状，逆操作一下
        # Pytorch 设置如果 transpose 后 view 会报错（因为view直接基于底层数组），需要调用 contiguous 来重新开辟一段内存
        x = x.transpose(1,2).contiguous().view(batch_size, -1, self.d_k)
        del query, key, value
        return self.fnn(x)
```

## 三、Transformer

### 1. 基础概念

1. Seq2Seq：Seq2Seq 是用 Encoder 将输入序列编码成 context，再使用 Decoder 将 context 解码成输出序列的过程。可以是文字-文字，也可以是文字-图像或者图像-文字均可。

    存在的问题：① 忽略了输入序列X的长度：当输入句子长度很长，特别是比训练集中最初的句子长度还长时，模型的性能急剧下降；② 对输入序列X缺乏区分度：输入X编码成一个固定的长度，对句子中每个词都赋予相同的权重，这样做没有区分度，往往使模型性能下降。

2. Transformer 结构：

    ![](figures/transformer.png)

    流程：① 输入文本经过 embedding 和位置编码转化为词向量；② 输入向量进入多头注意力层，得到注意力计算结果；③ 注意力计算结果与输入向量实现残差连接欸，经过层归一化输出；④ 输出结果通过全连接层，经过残差连接和层归一化作为该 Encoder 块的输出；⑤ 经过 N 个 Encoder 块输出编码 context；⑥ 输出文本经过 embedding、位置编码转化为输出向量；⑦ 输出向量以语言模型的形式逐个进入掩码多头自注意力，计算结果经过残差连接和层归一化进入多头注意力层；⑧ context 进入多头注意力层，与输出掩码自注意力计算结果计算注意力，在经过残差连接、层归一化、全连接、残差连接和层归一化得到输出；⑨ 经过 N 个 Decoder 块后得到解码结果，经过线性映射和 Softmax 得到输出序列。

3. 训练流程示例（注意，仅在训练过程是该流程，推理时解码器将会循环逐个推理）：

    ① 假设输入文本序列为 [BOS] Hello world [EOS] * n（n 为 batch_size）；
    
    ② 输入文本经过 N 个 Encoder 后得到 $E \in R^{n\times i\times d_{model}}$（此处我们假设输入维度和输出维度统一为 $d_{model}$，n 为 batch_size，i 为序列长度）；

    ③ 在解码器，输入目标文本序列 [BOS] 世界好 [EOS] * n，输入在每一个 Decoder 块会经过以下计算：
    
    ④ 输入目标经过编码转化为输入向量 $T \in R^{n \times i \times d_{model}}$，在掩码自注意力层，会生成遮蔽向量 $M \in R^{n \times i \times i}$，遮蔽向量其实是一个下三角矩阵；

    ⑤ 在掩码自注意力层，输入向量首先经过注意力分数计算得到权重 $A \in R^{n \times i \times i}$，然后权重经过遮蔽向量遮蔽去掉当前词之后的注意力分数，再按注意力计算得到该层输出 $D_1 \in R^{n \times i \times d_{model}}$

    ⑥ $D_1$ 接下来进入注意力层，同 $E$ 计算注意力：具体来说，是由 $D_1$ 作为 Q，$E$ 作为 K 和 V，计算过程不变；输出再经过全连接层，得到该块的输出 $D_2 \in R^{n \times i \times d_{model}}$

    ⑦ 经过 N 个 Decoder 后得到输出 $D_n \in R^{n \times i \times d_{model}}$，经过线性层映射和 Softmax 层解析得到预测输出，同真实标签计算损失反向传播即可。


